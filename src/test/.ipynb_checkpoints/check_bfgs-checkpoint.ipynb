{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting value: 31105.44\n",
      "starting gradient: [  1042.4   -553.4  31415.   -3390. ]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 3.765842\n",
      "         Iterations: 50\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 3.7658420706109825\n",
       " hess_inv: array([[ 0.0341594 , -0.06284135, -0.11080352, -0.17307656],\n",
       "       [-0.06284135,  0.11785987,  0.20779051,  0.3245233 ],\n",
       "       [-0.11080352,  0.20779051,  0.36757939,  0.5742851 ],\n",
       "       [-0.17307656,  0.3245233 ,  0.5742851 ,  0.89953395]])\n",
       "      jac: array([-0.87489381,  0.5960595 , -0.09132049, -0.08538006])\n",
       "  message: 'Maximum number of iterations has been exceeded.'\n",
       "     nfev: 61\n",
       "      nit: 50\n",
       "     njev: 61\n",
       "   status: 1\n",
       "  success: False\n",
       "        x: array([-0.90610456,  0.82912972,  0.68951563,  0.4750049 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cross check the BFGS algorithm against the python implementation\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "\n",
    "##x0 = [1.4, 0.2, 1.7]\n",
    "##x0 = [-1.2, 0.8, 1.0, 0.9]\n",
    "x0 = [1.4, 0.1, 4.5, 3.3]\n",
    "\n",
    "print('starting value:',rosen(x0))\n",
    "print('starting gradient:',rosen_der(x0))\n",
    "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
    "               options={'gtol': 1e-7, 'disp': True, 'maxiter': 50})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11963298118\n",
      "-0.125\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize.linesearch import _cubicmin, _quadmin\n",
    "\n",
    "print(_cubicmin(1, 1, 1, 2, 2, -1, 1))\n",
    "\n",
    "print(_quadmin(1, 1, 1, -2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import (atleast_1d, eye, mgrid, argmin, zeros, shape, squeeze,\n",
    "                   vectorize, asarray, sqrt, Inf, asfarray, isinf)\n",
    "import numpy as np\n",
    "from warnings import warn\n",
    "\n",
    "from scipy._lib.six import xrange\n",
    "\n",
    "class LineSearchWarning(RuntimeWarning):\n",
    "    pass\n",
    "\n",
    "class _LineSearchError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "_epsilon = sqrt(np.finfo(float).eps)\n",
    "\n",
    "# standard status messages of optimizers\n",
    "_status_message = {'success': 'Optimization terminated successfully.',\n",
    "                   'maxfev': 'Maximum number of function evaluations has '\n",
    "                              'been exceeded.',\n",
    "                   'maxiter': 'Maximum number of iterations has been '\n",
    "                              'exceeded.',\n",
    "                   'pr_loss': 'Desired error not necessarily achieved due '\n",
    "                              'to precision loss.'}\n",
    "\n",
    "class OptimizeResult(dict):\n",
    "    \"\"\" Represents the optimization result.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The solution of the optimization.\n",
    "    success : bool\n",
    "        Whether or not the optimizer exited successfully.\n",
    "    status : int\n",
    "        Termination status of the optimizer. Its value depends on the\n",
    "        underlying solver. Refer to `message` for details.\n",
    "    message : str\n",
    "        Description of the cause of the termination.\n",
    "    fun, jac, hess, hess_inv : ndarray\n",
    "        Values of objective function, Jacobian, Hessian or its inverse (if\n",
    "        available). The Hessians may be approximations, see the documentation\n",
    "        of the function in question.\n",
    "    nfev, njev, nhev : int\n",
    "        Number of evaluations of the objective functions and of its\n",
    "        Jacobian and Hessian.\n",
    "    nit : int\n",
    "        Number of iterations performed by the optimizer.\n",
    "    maxcv : float\n",
    "        The maximum constraint violation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    There may be additional attributes not listed above depending of the\n",
    "    specific solver. Since this class is essentially a subclass of dict\n",
    "    with attribute accessors, one can see which attributes are available\n",
    "    using the `keys()` method.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.keys():\n",
    "            m = max(map(len, list(self.keys()))) + 1\n",
    "            return '\\n'.join([k.rjust(m) + ': ' + repr(v)\n",
    "                              for k, v in self.items()])\n",
    "        else:\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class OptimizeWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "def fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-5, norm=Inf,\n",
    "              epsilon=_epsilon, maxiter=None, full_output=0, disp=1,\n",
    "              retall=0, callback=None):\n",
    "    \"\"\"\n",
    "    Minimize a function using the BFGS algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function to be minimized.\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    fprime : callable f'(x,*args), optional\n",
    "        Gradient of f.\n",
    "    args : tuple, optional\n",
    "        Extra arguments passed to f and fprime.\n",
    "    gtol : float, optional\n",
    "        Gradient norm must be less than gtol before successful termination.\n",
    "    norm : float, optional\n",
    "        Order of norm (Inf is max, -Inf is min)\n",
    "    epsilon : int or ndarray, optional\n",
    "        If fprime is approximated, use this value for the step size.\n",
    "    callback : callable, optional\n",
    "        An optional user-supplied function to call after each\n",
    "        iteration.  Called as callback(xk), where xk is the\n",
    "        current parameter vector.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    full_output : bool, optional\n",
    "        If True,return fopt, func_calls, grad_calls, and warnflag\n",
    "        in addition to xopt.\n",
    "    disp : bool, optional\n",
    "        Print convergence message if True.\n",
    "    retall : bool, optional\n",
    "        Return a list of results at each iteration if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xopt : ndarray\n",
    "        Parameters which minimize f, i.e. f(xopt) == fopt.\n",
    "    fopt : float\n",
    "        Minimum value.\n",
    "    gopt : ndarray\n",
    "        Value of gradient at minimum, f'(xopt), which should be near 0.\n",
    "    Bopt : ndarray\n",
    "        Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
    "    func_calls : int\n",
    "        Number of function_calls made.\n",
    "    grad_calls : int\n",
    "        Number of gradient calls made.\n",
    "    warnflag : integer\n",
    "        1 : Maximum number of iterations exceeded.\n",
    "        2 : Gradient and/or function calls not changing.\n",
    "    allvecs  :  list\n",
    "        `OptimizeResult` at each iteration.  Only returned if retall is True.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    minimize: Interface to minimization algorithms for multivariate\n",
    "        functions. See the 'BFGS' `method` in particular.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Optimize the function, f, whose gradient is given by fprime\n",
    "    using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
    "    and Shanno (BFGS)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
    "\n",
    "    \"\"\"\n",
    "    opts = {'gtol': gtol,\n",
    "            'norm': norm,\n",
    "            'eps': epsilon,\n",
    "            'disp': disp,\n",
    "            'maxiter': maxiter,\n",
    "            'return_all': retall}\n",
    "\n",
    "    res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
    "\n",
    "    if full_output:\n",
    "        retlist = (res['x'], res['fun'], res['jac'], res['hess_inv'],\n",
    "                   res['nfev'], res['njev'], res['status'])\n",
    "        if retall:\n",
    "            retlist += (res['allvecs'], )\n",
    "        return retlist\n",
    "    else:\n",
    "        if retall:\n",
    "            return res['x'], res['allvecs']\n",
    "        else:\n",
    "            return res['x']\n",
    "\n",
    "def _minimize_bfgs(fun, x0, args=(), jac=None, callback=None,\n",
    "                   gtol=1e-5, norm=Inf, eps=_epsilon, maxiter=None,\n",
    "                   disp=False, return_all=False,\n",
    "                   **unknown_options):\n",
    "    \"\"\"\n",
    "    Minimization of scalar function of one or more variables using the\n",
    "    BFGS algorithm.\n",
    "\n",
    "    Options\n",
    "    -------\n",
    "    disp : bool\n",
    "        Set to True to print convergence messages.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations to perform.\n",
    "    gtol : float\n",
    "        Gradient norm must be less than `gtol` before successful\n",
    "        termination.\n",
    "    norm : float\n",
    "        Order of norm (Inf is max, -Inf is min).\n",
    "    eps : float or ndarray\n",
    "        If `jac` is approximated, use this value for the step size.\n",
    "\n",
    "    \"\"\"\n",
    "    _check_unknown_options(unknown_options)\n",
    "    f = fun\n",
    "    fprime = jac\n",
    "    epsilon = eps\n",
    "    retall = return_all\n",
    "\n",
    "    x0 = asarray(x0).flatten()\n",
    "    if x0.ndim == 0:\n",
    "        x0.shape = (1,)\n",
    "    if maxiter is None:\n",
    "        maxiter = len(x0) * 200\n",
    "    func_calls, f = wrap_function(f, args)\n",
    "    if fprime is None:\n",
    "        grad_calls, myfprime = wrap_function(approx_fprime, (f, epsilon))\n",
    "    else:\n",
    "        grad_calls, myfprime = wrap_function(fprime, args)\n",
    "    gfk = myfprime(x0)\n",
    "    k = 0\n",
    "    N = len(x0)\n",
    "    I = np.eye(N, dtype=int)\n",
    "    Hk = I\n",
    "    old_fval = f(x0)\n",
    "    old_old_fval = None\n",
    "    xk = x0\n",
    "    if retall:\n",
    "        allvecs = [x0]\n",
    "    sk = [2 * gtol]\n",
    "    warnflag = 0\n",
    "    gnorm = vecnorm(gfk, ord=norm)\n",
    "    ##print('Function:', old_fval)\n",
    "    ##print('Gradient:', gfk)\n",
    "    while (gnorm > gtol) and (k < maxiter):\n",
    "        pk = -np.dot(Hk, gfk)\n",
    "        print('Direction:', pk)\n",
    "        try:\n",
    "            alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n",
    "                     _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n",
    "                                          old_fval, old_old_fval)\n",
    "        except _LineSearchError:\n",
    "            # Line search failed to find a better solution.\n",
    "            warnflag = 2\n",
    "            break\n",
    "\n",
    "        xkp1 = xk + alpha_k * pk\n",
    "        if retall:\n",
    "            allvecs.append(xkp1)\n",
    "        sk = xkp1 - xk\n",
    "        xk = xkp1\n",
    "        if gfkp1 is None:\n",
    "            gfkp1 = myfprime(xkp1)\n",
    "\n",
    "        yk = gfkp1 - gfk\n",
    "        gfk = gfkp1\n",
    "        if callback is not None:\n",
    "            callback(xk)\n",
    "        k += 1\n",
    "        gnorm = vecnorm(gfk, ord=norm)\n",
    "        if (gnorm <= gtol):\n",
    "            break\n",
    "\n",
    "        if not np.isfinite(old_fval):\n",
    "            # We correctly found +-Inf as optimal value, or something went\n",
    "            # wrong.\n",
    "            warnflag = 2\n",
    "            break\n",
    "        ##print(\"norm:\", np.dot(yk, sk))\n",
    "        try:  # this was handled in numeric, let it remaines for more safety\n",
    "            rhok = 1.0 / (np.dot(yk, sk))\n",
    "        except ZeroDivisionError:\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        if isinf(rhok):  # this is patch for numpy\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        ##print(\"sk\", sk)\n",
    "        ##print(\"yk\", yk)\n",
    "        A1 = I - sk[:, np.newaxis] * yk[np.newaxis, :] * rhok\n",
    "        A2 = I - yk[:, np.newaxis] * sk[np.newaxis, :] * rhok\n",
    "        Hk = np.dot(A1, np.dot(Hk, A2)) + (rhok * sk[:, np.newaxis] *\n",
    "                                                 sk[np.newaxis, :])\n",
    "        \n",
    "        print('Iteration:', k)\n",
    "        print('Function:', old_fval)\n",
    "        ##print('Gradient:', gfk)\n",
    "        ##print('Inverse approx', Hk)\n",
    "\n",
    "    fval = old_fval\n",
    "    if warnflag == 2:\n",
    "        msg = _status_message['pr_loss']\n",
    "        if disp:\n",
    "            print(\"Warning: \" + msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    elif k >= maxiter:\n",
    "        warnflag = 1\n",
    "        msg = _status_message['maxiter']\n",
    "        if disp:\n",
    "            print(\"Warning: \" + msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "    else:\n",
    "        msg = _status_message['success']\n",
    "        if disp:\n",
    "            print(msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    result = OptimizeResult(fun=fval, jac=gfk, hess_inv=Hk, nfev=func_calls[0],\n",
    "                            njev=grad_calls[0], status=warnflag,\n",
    "                            success=(warnflag == 0), message=msg, x=xk,\n",
    "                            nit=k)\n",
    "    if retall:\n",
    "        result['allvecs'] = allvecs\n",
    "    return result\n",
    "\n",
    "def vecnorm(x, ord=2):\n",
    "    if ord == Inf:\n",
    "        return np.amax(np.abs(x))\n",
    "    elif ord == -Inf:\n",
    "        return np.amin(np.abs(x))\n",
    "    else:\n",
    "        return np.sum(np.abs(x)**ord, axis=0)**(1.0 / ord)\n",
    "\n",
    "def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,\n",
    "                         **kwargs):\n",
    "    \"\"\"\n",
    "    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if\n",
    "    suitable step length is not found, and raise an exception if a\n",
    "    suitable step length is not found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    _LineSearchError\n",
    "        If no suitable step size is found\n",
    "\n",
    "    \"\"\"\n",
    "    ret = line_search_wolfe2(f, fprime, xk, pk, gfk,\n",
    "                             old_fval, old_old_fval)\n",
    "\n",
    "    if ret[0] is None:\n",
    "        raise _LineSearchError()\n",
    "\n",
    "    return ret\n",
    "\n",
    "def _check_unknown_options(unknown_options):\n",
    "    if unknown_options:\n",
    "        msg = \", \".join(map(str, unknown_options.keys()))\n",
    "        # Stack level 4: this is called from _minimize_*, which is\n",
    "        # called from another function in Scipy. Level 4 is the first\n",
    "        # level in user code.\n",
    "        warnings.warn(\"Unknown solver options: %s\" % msg, OptimizeWarning, 4)\n",
    "\n",
    "def wrap_function(function, args):\n",
    "    ncalls = [0]\n",
    "    if function is None:\n",
    "        return ncalls, None\n",
    "\n",
    "    def function_wrapper(*wrapper_args):\n",
    "        ncalls[0] += 1\n",
    "        return function(*(wrapper_args + args))\n",
    "\n",
    "    return ncalls, function_wrapper \n",
    "\n",
    "def line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None,\n",
    "                       old_old_fval=None, args=(), c1=1e-4, c2=0.9, amax=50):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function.\n",
    "    myfprime : callable f'(x,*args)\n",
    "        Objective function gradient.\n",
    "    xk : ndarray\n",
    "        Starting point.\n",
    "    pk : ndarray\n",
    "        Search direction.\n",
    "    gfk : ndarray, optional\n",
    "        Gradient value for x=xk (xk being the current parameter\n",
    "        estimate). Will be recomputed if omitted.\n",
    "    old_fval : float, optional\n",
    "        Function value for x=xk. Will be recomputed if omitted.\n",
    "    old_old_fval : float, optional\n",
    "        Function value for the point preceding x=xk\n",
    "    args : tuple, optional\n",
    "        Additional arguments passed to objective function.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float or None\n",
    "        Alpha for which ``x_new = x0 + alpha * pk``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    fc : int\n",
    "        Number of function evaluations made.\n",
    "    gc : int\n",
    "        Number of gradient evaluations made.\n",
    "    new_fval : float or None\n",
    "        New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    old_fval : float\n",
    "        Old function value ``f(x0)``.\n",
    "    new_slope : float or None\n",
    "        The local slope along the search direction at the\n",
    "        new value ``<myfprime(x_new), pk>``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "    fc = [0]\n",
    "    gc = [0]\n",
    "    gval = [None]\n",
    "\n",
    "    def phi(alpha):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha * pk, *args)\n",
    "\n",
    "    if isinstance(myfprime, tuple):\n",
    "        def derphi(alpha):\n",
    "            fc[0] += len(xk) + 1\n",
    "            eps = myfprime[1]\n",
    "            fprime = myfprime[0]\n",
    "            newargs = (f, eps) + args\n",
    "            gval[0] = fprime(xk + alpha * pk, *newargs)  # store for later use\n",
    "            return np.dot(gval[0], pk)\n",
    "    else:\n",
    "        fprime = myfprime\n",
    "\n",
    "        def derphi(alpha):\n",
    "            gc[0] += 1\n",
    "            gval[0] = fprime(xk + alpha * pk, *args)  # store for later use\n",
    "            return np.dot(gval[0], pk)\n",
    "\n",
    "    if gfk is None:\n",
    "        gfk = fprime(xk, *args)\n",
    "    derphi0 = np.dot(gfk, pk)\n",
    "\n",
    "    alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n",
    "            phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax)\n",
    "\n",
    "    if derphi_star is None:\n",
    "        print(\"HERE1\")\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "    else:\n",
    "        # derphi_star is a number (derphi) -- so use the most recently\n",
    "        # calculated gradient used in computing it derphi = gfk*pk\n",
    "        # this is the gradient at the next step no need to compute it\n",
    "        # again in the outer loop.\n",
    "        derphi_star = gval[0]\n",
    "\n",
    "    return alpha_star, fc[0], gc[0], phi_star, old_fval, derphi_star\n",
    "\n",
    "\n",
    "def scalar_search_wolfe2(phi, derphi=None, phi0=None,\n",
    "                         old_phi0=None, derphi0=None,\n",
    "                         c1=1e-4, c2=0.9, amax=50):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable f(x)\n",
    "        Objective scalar function.\n",
    "    derphi : callable f'(x), optional\n",
    "        Objective function derivative (can be None)\n",
    "    phi0 : float, optional\n",
    "        Value of phi at s=0\n",
    "    old_phi0 : float, optional\n",
    "        Value of phi at previous point\n",
    "    derphi0 : float, optional\n",
    "        Value of derphi at s=0\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha_star : float or None\n",
    "        Best alpha, or None if the line search algorithm did not converge.\n",
    "    phi_star : float\n",
    "        phi at alpha_star\n",
    "    phi0 : float\n",
    "        phi at 0\n",
    "    derphi_star : float or None\n",
    "        derphi at alpha_star, or None if the line search algorithm\n",
    "        did not converge.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "    ##print('CALLING scalar_search_wolfe2')\n",
    "    if phi0 is None:\n",
    "        phi0 = phi(0.)\n",
    "\n",
    "    if derphi0 is None and derphi is not None:\n",
    "        derphi0 = derphi(0.)\n",
    "\n",
    "    alpha0 = 0\n",
    "    if old_phi0 is not None and derphi0 != 0:\n",
    "        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
    "    else:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 < 0:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 == 0:\n",
    "        # This shouldn't happen. Perhaps the increment has slipped below\n",
    "        # machine precision?  For now, set the return variables skip the\n",
    "        # useless while loop, and raise warnflag=2 due to possible imprecision.\n",
    "        alpha_star = None\n",
    "        phi_star = phi0\n",
    "        phi0 = old_phi0\n",
    "        derphi_star = None\n",
    "\n",
    "    phi_a1 = phi(alpha1)\n",
    "    #derphi_a1 = derphi(alpha1)  evaluated below\n",
    "\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "\n",
    "    ##print('phi0:', phi0)\n",
    "    ##print('derphi0:', derphi0)\n",
    "    \n",
    "    i = 1\n",
    "    maxiter = 10\n",
    "    ##print('alpha0:', alpha0)\n",
    "    ##print('alpha1:', alpha1)\n",
    "    ##print('phi_a0:', phi_a0)\n",
    "    ##print('derphi_a0:', derphi_a0)\n",
    "    ##print('phi_a1:', phi_a1)\n",
    "    \n",
    "    for i in xrange(maxiter):\n",
    "        if alpha1 == 0:\n",
    "            break\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or \\\n",
    "           ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha0, alpha1, phi_a0,\n",
    "                              phi_a1, derphi_a0, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            print('ZOOM1')\n",
    "            print('alpha star:', alpha_star)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= -c2*derphi0):\n",
    "            alpha_star = alpha1\n",
    "            phi_star = phi_a1\n",
    "            derphi_star = derphi_a1\n",
    "            print('NO ZOOM')\n",
    "            break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha1, alpha0, phi_a1,\n",
    "                              phi_a0, derphi_a1, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            print('ZOOM2')\n",
    "            print('alpha star:', alpha_star)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1   # increase by factor of two on each iteration\n",
    "        i = i + 1\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        print(\"HERE\")\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "\n",
    "    return alpha_star, phi_star, phi0, derphi_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a cubic polynomial that goes through the\n",
    "    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n",
    "\n",
    "    If no minimizer can be found return None\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a quadratic polynomial that goes through\n",
    "    the points (a,fa), (b,fb) with derivative at a of fpa,\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "          phi, derphi, phi0, derphi0, c1, c2):\n",
    "    \"\"\"\n",
    "    Part of the optimization algorithm in `scalar_search_wolfe2`.\n",
    "    \"\"\"\n",
    "\n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        # interpolate to find a trial step length between a_lo and\n",
    "        # a_hi Need to choose interpolation here.  Use cubic\n",
    "        # interpolation and then if the result is within delta *\n",
    "        # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "        # then use quadratic interpolation, if the result is still too\n",
    "        # close, then use bisection\n",
    "\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "\n",
    "        # minimizer of cubic interpolant\n",
    "        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n",
    "        #\n",
    "        # if the result is too close to the end points (or out of the\n",
    "        # interval) then use quadratic interpolation with phi_lo,\n",
    "        # derphi_lo and phi_hi if the result is stil too close to the\n",
    "        # end points (or out of the interval) then use bisection\n",
    "\n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                            a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "\n",
    "        # Check new value of a_j\n",
    "\n",
    "        phi_aj = phi(a_j)\n",
    "        print(\"optloss \", phi_aj)\n",
    "        print(\"phi0 + c1*a_j*derphi0 \", phi0 + c1*a_j*derphi0)\n",
    "        print(\"phi_lo \", phi_lo)\n",
    "        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if abs(derphi_aj) <= -c2*derphi0:\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            print(\"HERE2\")\n",
    "            break\n",
    "    return a_star, val_star, valprime_star       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction: [ -1042.4    513.4 -13233.    1790. ]\n",
      "optloss  1.9124671825e+17\n",
      "phi0 + c1*a_j*derphi0  598.111684\n",
      "phi_lo  9581.44\n",
      "optloss  7.4634781409e+15\n",
      "phi0 + c1*a_j*derphi0  5585.98042141\n",
      "phi_lo  9581.44\n",
      "optloss  3.38470364703e+14\n",
      "phi0 + c1*a_j*derphi0  7735.03969834\n",
      "phi_lo  9581.44\n",
      "optloss  1.46394806481e+13\n",
      "phi0 + c1*a_j*derphi0  8736.77327706\n",
      "phi_lo  9581.44\n",
      "optloss  641769389801.0\n",
      "phi0 + c1*a_j*derphi0  9192.30724182\n",
      "phi_lo  9581.44\n",
      "optloss  27921164136.7\n",
      "phi0 + c1*a_j*derphi0  9401.07500489\n",
      "phi_lo  9581.44\n",
      "optloss  1206649869.71\n",
      "phi0 + c1*a_j*derphi0  9496.53892097\n",
      "phi_lo  9581.44\n",
      "optloss  50838193.8291\n",
      "phi0 + c1*a_j*derphi0  9540.26074813\n",
      "phi_lo  9581.44\n",
      "optloss  1958319.05633\n",
      "phi0 + c1*a_j*derphi0  9560.35181411\n",
      "phi_lo  9581.44\n",
      "optloss  50684.6567139\n",
      "phi0 + c1*a_j*derphi0  9569.72062151\n",
      "phi_lo  9581.44\n",
      "optloss  497.827122462\n",
      "phi0 + c1*a_j*derphi0  9574.37694222\n",
      "phi_lo  9581.44\n",
      "ZOOM1\n",
      "alpha star: 0.000393120318611\n",
      "Iteration: 1\n",
      "Function: 497.827122462\n",
      "Direction: [-247.55498306  -95.88522245  -33.086338   -277.26905608]\n",
      "optloss  4634051.27312\n",
      "phi0 + c1*a_j*derphi0  496.909677561\n",
      "phi_lo  497.827122462\n",
      "optloss  188292.9184\n",
      "phi0 + c1*a_j*derphi0  497.395867189\n",
      "phi_lo  497.827122462\n",
      "optloss  9479.01570954\n",
      "phi0 + c1*a_j*derphi0  497.612617923\n",
      "phi_lo  497.827122462\n",
      "optloss  873.342328224\n",
      "phi0 + c1*a_j*derphi0  497.719778524\n",
      "phi_lo  497.827122462\n",
      "optloss  350.116678062\n",
      "phi0 + c1*a_j*derphi0  497.774488551\n",
      "phi_lo  497.827122462\n",
      "ZOOM1\n",
      "alpha star: 0.00354722320931\n",
      "Iteration: 2\n",
      "Function: 350.116678062\n",
      "Direction: [-95.96510423  25.19509039  12.8920926   37.09310648]\n",
      "optloss  527.186943609\n",
      "phi0 + c1*a_j*derphi0  350.101759307\n",
      "phi_lo  350.116678062\n",
      "optloss  327.680612995\n",
      "phi0 + c1*a_j*derphi0  350.1091925\n",
      "phi_lo  350.116678062\n",
      "ZOOM1\n",
      "alpha star: 0.00698990511759\n",
      "Iteration: 3\n",
      "Function: 327.680612995\n",
      "Direction: [ -5.51181145 -18.44402141   1.0873172    9.54786443]\n",
      "optloss  766.440080382\n",
      "phi0 + c1*a_j*derphi0  327.678346953\n",
      "phi_lo  327.680612995\n",
      "optloss  425.013426707\n",
      "phi0 + c1*a_j*derphi0  327.679479974\n",
      "phi_lo  327.680612995\n",
      "optloss  348.975827669\n",
      "phi0 + c1*a_j*derphi0  327.680046485\n",
      "phi_lo  327.680612995\n",
      "optloss  327.384922504\n",
      "phi0 + c1*a_j*derphi0  327.680553476\n",
      "phi_lo  327.680612995\n",
      "ZOOM1\n",
      "alpha star: 0.00121750451672\n",
      "Iteration: 4\n",
      "Function: 327.384922504\n",
      "Direction: [ 0.3938735  -0.14746977  0.03577324 -0.25980694]\n",
      "NO ZOOM\n",
      "Iteration: 5\n",
      "Function: 322.868992504\n",
      "Direction: [ 0.29550111 -0.12016994  0.1516491  -0.75987621]\n",
      "NO ZOOM\n",
      "Iteration: 6\n",
      "Function: 289.892448109\n",
      "Direction: [ 0.56640131 -0.1863963   0.51238028 -2.15837819]\n",
      "NO ZOOM\n",
      "Iteration: 7\n",
      "Function: 230.730472864\n",
      "Direction: [ 0.72830115 -0.17415712  0.77851465 -2.85859741]\n",
      "NO ZOOM\n",
      "Iteration: 8\n",
      "Function: 148.497268306\n",
      "Direction: [ 0.22970548 -0.01419543  0.26233438 -0.65263859]\n",
      "NO ZOOM\n",
      "Iteration: 9\n",
      "Function: 116.513326937\n",
      "Direction: [ 0.0403992   0.08126428  0.18385626 -0.06652429]\n",
      "NO ZOOM\n",
      "Iteration: 10\n",
      "Function: 65.0685535657\n",
      "Direction: [-0.02792741  0.1310172   0.26136954 -0.01071685]\n",
      "NO ZOOM\n",
      "Iteration: 11\n",
      "Function: 20.6361964454\n",
      "Direction: [ 0.05780621  0.0615888   0.27959869 -0.23780346]\n",
      "NO ZOOM\n",
      "Iteration: 12\n",
      "Function: 8.39920917306\n",
      "Direction: [-0.02032372  0.03146453  0.08807954  0.21965087]\n",
      "NO ZOOM\n",
      "Iteration: 13\n",
      "Function: 2.46985102685\n",
      "Direction: [ 0.01380572  0.01558848  0.07250987 -0.04356689]\n",
      "NO ZOOM\n",
      "Iteration: 14\n",
      "Function: 1.86449991935\n",
      "Direction: [ 0.00205246  0.00286715  0.00426946  0.0117467 ]\n",
      "NO ZOOM\n",
      "Iteration: 15\n",
      "Function: 1.83571058649\n",
      "Direction: [ 0.00452992  0.00497479  0.00571038  0.00603783]\n",
      "NO ZOOM\n",
      "Iteration: 16\n",
      "Function: 1.8209528315\n",
      "Direction: [ 0.01227082  0.01239309  0.0095795   0.00760608]\n",
      "NO ZOOM\n",
      "Iteration: 17\n",
      "Function: 1.7953465732\n",
      "Direction: [ 0.04813269  0.04747371  0.03069459  0.01807146]\n",
      "NO ZOOM\n",
      "Iteration: 18\n",
      "Function: 1.74722876525\n",
      "Direction: [ 0.20931792  0.20826532  0.1269396   0.05924163]\n",
      "NO ZOOM\n",
      "Iteration: 19\n",
      "Function: 1.3612968012\n",
      "Direction: [ 2.63475367  2.75051162  1.73045934  0.70408692]\n",
      "optloss  1.53102938443\n",
      "phi0 + c1*a_j*derphi0  1.36125782207\n",
      "phi_lo  1.3612968012\n",
      "optloss  1.24917946119\n",
      "phi0 + c1*a_j*derphi0  1.36127751482\n",
      "phi_lo  1.3612968012\n",
      "ZOOM1\n",
      "alpha star: 0.0284328252849\n",
      "Iteration: 20\n",
      "Function: 1.24917946119\n",
      "Direction: [ 0.02381368  0.04032396  0.02909045  0.00433141]\n",
      "NO ZOOM\n",
      "Iteration: 21\n",
      "Function: 1.06383109122\n",
      "Direction: [ 0.0450604   0.10864869  0.10930049 -0.00681409]\n",
      "NO ZOOM\n",
      "Iteration: 22\n",
      "Function: 0.800729309003\n",
      "Direction: [ 0.00923724  0.02928995  0.03060282  0.00288536]\n",
      "NO ZOOM\n",
      "Iteration: 23\n",
      "Function: 0.746226036025\n",
      "Direction: [-0.00211194 -0.00170011  0.00224204  0.00323317]\n",
      "NO ZOOM\n",
      "Iteration: 24\n",
      "Function: 0.727183870935\n",
      "Direction: [ 0.04681275  0.06309098  0.09230614  0.06750119]\n",
      "NO ZOOM\n",
      "Iteration: 25\n",
      "Function: 0.690742523635\n",
      "Direction: [ 0.07354119  0.11835483  0.15111087  0.12516681]\n",
      "NO ZOOM\n",
      "Iteration: 26\n",
      "Function: 0.622761526979\n",
      "Direction: [ 0.09016938  0.13705207  0.1726413   0.13310736]\n",
      "NO ZOOM\n",
      "Iteration: 27\n",
      "Function: 0.507260840482\n",
      "Direction: [ 0.06501812  0.11113333  0.14771462  0.14213419]\n",
      "NO ZOOM\n",
      "Iteration: 28\n",
      "Function: 0.345251572136\n",
      "Direction: [ 0.02535094  0.04798018  0.08238444  0.10860255]\n",
      "NO ZOOM\n",
      "Iteration: 29\n",
      "Function: 0.206464292666\n",
      "Direction: [ 0.08288152  0.13619101  0.21178732  0.22968405]\n",
      "optloss  0.178857993269\n",
      "phi0 + c1*a_j*derphi0  0.206460947931\n",
      "phi_lo  0.206464292666\n",
      "ZOOM1\n",
      "alpha star: 0.146933690302\n",
      "Iteration: 30\n",
      "Function: 0.178857993269\n",
      "Direction: [ 0.04473432  0.07550263  0.12549237  0.16124046]\n",
      "NO ZOOM\n",
      "Iteration: 31\n",
      "Function: 0.135416454338\n",
      "Direction: [ 0.02582009  0.04598031  0.08230068  0.11925066]\n",
      "NO ZOOM\n",
      "Iteration: 32\n",
      "Function: 0.0853146357292\n",
      "Direction: [ 0.00885356  0.01679234  0.03547461  0.06977746]\n",
      "NO ZOOM\n",
      "Iteration: 33\n",
      "Function: 0.0455047053696\n",
      "Direction: [ 0.0274076   0.05907622  0.09971249  0.16174109]\n",
      "optloss  0.024416229512\n",
      "phi0 + c1*a_j*derphi0  0.0455014439091\n",
      "phi_lo  0.0349428226239\n",
      "ZOOM2\n",
      "alpha star: 0.568186353448\n",
      "Iteration: 34\n",
      "Function: 0.024416229512\n",
      "Direction: [ 0.00956339  0.02002565  0.03826535  0.06943436]\n",
      "NO ZOOM\n",
      "Iteration: 35\n",
      "Function: 0.013048867471\n",
      "Direction: [ 0.00738121  0.01414851  0.02899497  0.06117682]\n",
      "NO ZOOM\n",
      "Iteration: 36\n",
      "Function: 0.00424215226814\n",
      "Direction: [ 0.01660253  0.02990271  0.0595372   0.10773781]\n",
      "optloss  0.00126136903602\n",
      "phi0 + c1*a_j*derphi0  0.00424167430519\n",
      "phi_lo  0.00424215226814\n",
      "ZOOM1\n",
      "alpha star: 0.44933650727\n",
      "Iteration: 37\n",
      "Function: 0.00126136903602\n",
      "Direction: [ 0.00366457  0.00841407  0.01748978  0.03396937]\n",
      "NO ZOOM\n",
      "Iteration: 38\n",
      "Function: 0.000447107785825\n",
      "Direction: [ 0.0022284   0.00397457  0.00730923  0.01583218]\n",
      "NO ZOOM\n",
      "Iteration: 39\n",
      "Function: 4.01643355606e-05\n",
      "Direction: [ -9.43315844e-06   2.06272999e-04   1.05218386e-03   2.76319172e-03]\n",
      "NO ZOOM\n",
      "Iteration: 40\n",
      "Function: 1.98275002014e-05\n",
      "Direction: [ 0.00058207  0.00105403  0.00183415  0.00339286]\n",
      "NO ZOOM\n",
      "Iteration: 41\n",
      "Function: 4.77025250274e-08\n",
      "Direction: [ -3.74779509e-05  -6.26757063e-05  -1.34027712e-04  -2.55377188e-04]\n",
      "NO ZOOM\n",
      "Iteration: 42\n",
      "Function: 1.26223222192e-09\n",
      "Direction: [  3.24096623e-06   3.93813925e-06   1.00576333e-05   1.92878025e-05]\n",
      "NO ZOOM\n",
      "Iteration: 43\n",
      "Function: 2.70203437647e-12\n",
      "Direction: [  1.62279494e-07   4.29375053e-07   7.75681976e-07   1.48777549e-06]\n",
      "NO ZOOM\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 44\n",
      "         Function evaluations: 79\n",
      "         Gradient evaluations: 53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00000001,  1.00000003,  1.00000006,  1.00000012])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [1.4, 0.1, 3.5, 3.3]\n",
    "fmin_bfgs(rosen, x0, rosen_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
