{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting value: 59.8\n",
      "starting gradient: [-311.6 -243.6  112.   -20. ]\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 3.701429\n",
      "         Iterations: 28\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        x: array([-0.77565923,  0.61309337,  0.38206285,  0.14597202])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 80\n",
       "      nit: 28\n",
       "     njev: 68\n",
       " hess_inv: array([[ 0.27763187, -0.42522815, -0.51666592, -0.39470104],\n",
       "       [-0.42522815,  0.65616383,  0.79730319,  0.60918797],\n",
       "       [-0.51666592,  0.79730319,  0.97364049,  0.74391376],\n",
       "       [-0.39470104,  0.60918797,  0.74391376,  0.57319309]])\n",
       "   status: 2\n",
       "      fun: 3.701428610430017\n",
       "  success: False\n",
       "      jac: array([  4.45409176e-09,  -9.65049796e-09,  -1.13726280e-08,\n",
       "         2.88341351e-08])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cross check the BFGS algorithm against the python implementation\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "\n",
    "##x0 = [1.4, 0.2, 1.7]\n",
    "x0 = [-1.2, 0.8, 1.0, 0.9]\n",
    "\n",
    "print('starting value:',rosen(x0))\n",
    "print('starting gradient:',rosen_der(x0))\n",
    "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
    "               options={'gtol': 1e-10, 'disp': True, 'maxiter': 500})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import (atleast_1d, eye, mgrid, argmin, zeros, shape, squeeze,\n",
    "                   vectorize, asarray, sqrt, Inf, asfarray, isinf)\n",
    "import numpy as np\n",
    "\n",
    "from scipy._lib.six import xrange\n",
    "\n",
    "class LineSearchWarning(RuntimeWarning):\n",
    "    pass\n",
    "\n",
    "class _LineSearchError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "_epsilon = sqrt(np.finfo(float).eps)\n",
    "\n",
    "# standard status messages of optimizers\n",
    "_status_message = {'success': 'Optimization terminated successfully.',\n",
    "                   'maxfev': 'Maximum number of function evaluations has '\n",
    "                              'been exceeded.',\n",
    "                   'maxiter': 'Maximum number of iterations has been '\n",
    "                              'exceeded.',\n",
    "                   'pr_loss': 'Desired error not necessarily achieved due '\n",
    "                              'to precision loss.'}\n",
    "\n",
    "class OptimizeResult(dict):\n",
    "    \"\"\" Represents the optimization result.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        The solution of the optimization.\n",
    "    success : bool\n",
    "        Whether or not the optimizer exited successfully.\n",
    "    status : int\n",
    "        Termination status of the optimizer. Its value depends on the\n",
    "        underlying solver. Refer to `message` for details.\n",
    "    message : str\n",
    "        Description of the cause of the termination.\n",
    "    fun, jac, hess, hess_inv : ndarray\n",
    "        Values of objective function, Jacobian, Hessian or its inverse (if\n",
    "        available). The Hessians may be approximations, see the documentation\n",
    "        of the function in question.\n",
    "    nfev, njev, nhev : int\n",
    "        Number of evaluations of the objective functions and of its\n",
    "        Jacobian and Hessian.\n",
    "    nit : int\n",
    "        Number of iterations performed by the optimizer.\n",
    "    maxcv : float\n",
    "        The maximum constraint violation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    There may be additional attributes not listed above depending of the\n",
    "    specific solver. Since this class is essentially a subclass of dict\n",
    "    with attribute accessors, one can see which attributes are available\n",
    "    using the `keys()` method.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.keys():\n",
    "            m = max(map(len, list(self.keys()))) + 1\n",
    "            return '\\n'.join([k.rjust(m) + ': ' + repr(v)\n",
    "                              for k, v in self.items()])\n",
    "        else:\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class OptimizeWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "def fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-5, norm=Inf,\n",
    "              epsilon=_epsilon, maxiter=None, full_output=0, disp=1,\n",
    "              retall=0, callback=None):\n",
    "    \"\"\"\n",
    "    Minimize a function using the BFGS algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function to be minimized.\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    fprime : callable f'(x,*args), optional\n",
    "        Gradient of f.\n",
    "    args : tuple, optional\n",
    "        Extra arguments passed to f and fprime.\n",
    "    gtol : float, optional\n",
    "        Gradient norm must be less than gtol before successful termination.\n",
    "    norm : float, optional\n",
    "        Order of norm (Inf is max, -Inf is min)\n",
    "    epsilon : int or ndarray, optional\n",
    "        If fprime is approximated, use this value for the step size.\n",
    "    callback : callable, optional\n",
    "        An optional user-supplied function to call after each\n",
    "        iteration.  Called as callback(xk), where xk is the\n",
    "        current parameter vector.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    full_output : bool, optional\n",
    "        If True,return fopt, func_calls, grad_calls, and warnflag\n",
    "        in addition to xopt.\n",
    "    disp : bool, optional\n",
    "        Print convergence message if True.\n",
    "    retall : bool, optional\n",
    "        Return a list of results at each iteration if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xopt : ndarray\n",
    "        Parameters which minimize f, i.e. f(xopt) == fopt.\n",
    "    fopt : float\n",
    "        Minimum value.\n",
    "    gopt : ndarray\n",
    "        Value of gradient at minimum, f'(xopt), which should be near 0.\n",
    "    Bopt : ndarray\n",
    "        Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
    "    func_calls : int\n",
    "        Number of function_calls made.\n",
    "    grad_calls : int\n",
    "        Number of gradient calls made.\n",
    "    warnflag : integer\n",
    "        1 : Maximum number of iterations exceeded.\n",
    "        2 : Gradient and/or function calls not changing.\n",
    "    allvecs  :  list\n",
    "        `OptimizeResult` at each iteration.  Only returned if retall is True.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    minimize: Interface to minimization algorithms for multivariate\n",
    "        functions. See the 'BFGS' `method` in particular.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Optimize the function, f, whose gradient is given by fprime\n",
    "    using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
    "    and Shanno (BFGS)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
    "\n",
    "    \"\"\"\n",
    "    opts = {'gtol': gtol,\n",
    "            'norm': norm,\n",
    "            'eps': epsilon,\n",
    "            'disp': disp,\n",
    "            'maxiter': maxiter,\n",
    "            'return_all': retall}\n",
    "\n",
    "    res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
    "\n",
    "    if full_output:\n",
    "        retlist = (res['x'], res['fun'], res['jac'], res['hess_inv'],\n",
    "                   res['nfev'], res['njev'], res['status'])\n",
    "        if retall:\n",
    "            retlist += (res['allvecs'], )\n",
    "        return retlist\n",
    "    else:\n",
    "        if retall:\n",
    "            return res['x'], res['allvecs']\n",
    "        else:\n",
    "            return res['x']\n",
    "\n",
    "def _minimize_bfgs(fun, x0, args=(), jac=None, callback=None,\n",
    "                   gtol=1e-5, norm=Inf, eps=_epsilon, maxiter=None,\n",
    "                   disp=False, return_all=False,\n",
    "                   **unknown_options):\n",
    "    \"\"\"\n",
    "    Minimization of scalar function of one or more variables using the\n",
    "    BFGS algorithm.\n",
    "\n",
    "    Options\n",
    "    -------\n",
    "    disp : bool\n",
    "        Set to True to print convergence messages.\n",
    "    maxiter : int\n",
    "        Maximum number of iterations to perform.\n",
    "    gtol : float\n",
    "        Gradient norm must be less than `gtol` before successful\n",
    "        termination.\n",
    "    norm : float\n",
    "        Order of norm (Inf is max, -Inf is min).\n",
    "    eps : float or ndarray\n",
    "        If `jac` is approximated, use this value for the step size.\n",
    "\n",
    "    \"\"\"\n",
    "    _check_unknown_options(unknown_options)\n",
    "    f = fun\n",
    "    fprime = jac\n",
    "    epsilon = eps\n",
    "    retall = return_all\n",
    "\n",
    "    x0 = asarray(x0).flatten()\n",
    "    if x0.ndim == 0:\n",
    "        x0.shape = (1,)\n",
    "    if maxiter is None:\n",
    "        maxiter = len(x0) * 200\n",
    "    func_calls, f = wrap_function(f, args)\n",
    "    if fprime is None:\n",
    "        grad_calls, myfprime = wrap_function(approx_fprime, (f, epsilon))\n",
    "    else:\n",
    "        grad_calls, myfprime = wrap_function(fprime, args)\n",
    "    gfk = myfprime(x0)\n",
    "    k = 0\n",
    "    N = len(x0)\n",
    "    I = np.eye(N, dtype=int)\n",
    "    Hk = I\n",
    "    old_fval = f(x0)\n",
    "    old_old_fval = None\n",
    "    xk = x0\n",
    "    if retall:\n",
    "        allvecs = [x0]\n",
    "    sk = [2 * gtol]\n",
    "    warnflag = 0\n",
    "    gnorm = vecnorm(gfk, ord=norm)\n",
    "    ##print('Function:', old_fval)\n",
    "    ##print('Gradient:', gfk)\n",
    "    while (gnorm > gtol) and (k < maxiter):\n",
    "        pk = -np.dot(Hk, gfk)\n",
    "        print('Direction:', pk)\n",
    "        try:\n",
    "            alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 = \\\n",
    "                     _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n",
    "                                          old_fval, old_old_fval)\n",
    "        except _LineSearchError:\n",
    "            # Line search failed to find a better solution.\n",
    "            warnflag = 2\n",
    "            break\n",
    "\n",
    "        xkp1 = xk + alpha_k * pk\n",
    "        if retall:\n",
    "            allvecs.append(xkp1)\n",
    "        sk = xkp1 - xk\n",
    "        xk = xkp1\n",
    "        if gfkp1 is None:\n",
    "            gfkp1 = myfprime(xkp1)\n",
    "\n",
    "        yk = gfkp1 - gfk\n",
    "        gfk = gfkp1\n",
    "        if callback is not None:\n",
    "            callback(xk)\n",
    "        k += 1\n",
    "        gnorm = vecnorm(gfk, ord=norm)\n",
    "        if (gnorm <= gtol):\n",
    "            break\n",
    "\n",
    "        if not np.isfinite(old_fval):\n",
    "            # We correctly found +-Inf as optimal value, or something went\n",
    "            # wrong.\n",
    "            warnflag = 2\n",
    "            break\n",
    "        ##print(\"norm:\", np.dot(yk, sk))\n",
    "        try:  # this was handled in numeric, let it remaines for more safety\n",
    "            rhok = 1.0 / (np.dot(yk, sk))\n",
    "        except ZeroDivisionError:\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        if isinf(rhok):  # this is patch for numpy\n",
    "            rhok = 1000.0\n",
    "            if disp:\n",
    "                print(\"Divide-by-zero encountered: rhok assumed large\")\n",
    "        ##print(\"sk\", sk)\n",
    "        ##print(\"yk\", yk)\n",
    "        A1 = I - sk[:, np.newaxis] * yk[np.newaxis, :] * rhok\n",
    "        A2 = I - yk[:, np.newaxis] * sk[np.newaxis, :] * rhok\n",
    "        Hk = np.dot(A1, np.dot(Hk, A2)) + (rhok * sk[:, np.newaxis] *\n",
    "                                                 sk[np.newaxis, :])\n",
    "        \n",
    "        print('Iteration:', k)\n",
    "        print('Function:', old_fval)\n",
    "        ##print('Gradient:', gfk)\n",
    "        ##print('Inverse approx', Hk)\n",
    "\n",
    "    fval = old_fval\n",
    "    if warnflag == 2:\n",
    "        msg = _status_message['pr_loss']\n",
    "        if disp:\n",
    "            print(\"Warning: \" + msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    elif k >= maxiter:\n",
    "        warnflag = 1\n",
    "        msg = _status_message['maxiter']\n",
    "        if disp:\n",
    "            print(\"Warning: \" + msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "    else:\n",
    "        msg = _status_message['success']\n",
    "        if disp:\n",
    "            print(msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "            print(\"         Function evaluations: %d\" % func_calls[0])\n",
    "            print(\"         Gradient evaluations: %d\" % grad_calls[0])\n",
    "\n",
    "    result = OptimizeResult(fun=fval, jac=gfk, hess_inv=Hk, nfev=func_calls[0],\n",
    "                            njev=grad_calls[0], status=warnflag,\n",
    "                            success=(warnflag == 0), message=msg, x=xk,\n",
    "                            nit=k)\n",
    "    if retall:\n",
    "        result['allvecs'] = allvecs\n",
    "    return result\n",
    "\n",
    "def vecnorm(x, ord=2):\n",
    "    if ord == Inf:\n",
    "        return np.amax(np.abs(x))\n",
    "    elif ord == -Inf:\n",
    "        return np.amin(np.abs(x))\n",
    "    else:\n",
    "        return np.sum(np.abs(x)**ord, axis=0)**(1.0 / ord)\n",
    "\n",
    "def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,\n",
    "                         **kwargs):\n",
    "    \"\"\"\n",
    "    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if\n",
    "    suitable step length is not found, and raise an exception if a\n",
    "    suitable step length is not found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    _LineSearchError\n",
    "        If no suitable step size is found\n",
    "\n",
    "    \"\"\"\n",
    "    ret = line_search_wolfe2(f, fprime, xk, pk, gfk,\n",
    "                             old_fval, old_old_fval)\n",
    "\n",
    "    if ret[0] is None:\n",
    "        raise _LineSearchError()\n",
    "\n",
    "    return ret\n",
    "\n",
    "def _check_unknown_options(unknown_options):\n",
    "    if unknown_options:\n",
    "        msg = \", \".join(map(str, unknown_options.keys()))\n",
    "        # Stack level 4: this is called from _minimize_*, which is\n",
    "        # called from another function in Scipy. Level 4 is the first\n",
    "        # level in user code.\n",
    "        warnings.warn(\"Unknown solver options: %s\" % msg, OptimizeWarning, 4)\n",
    "\n",
    "def wrap_function(function, args):\n",
    "    ncalls = [0]\n",
    "    if function is None:\n",
    "        return ncalls, None\n",
    "\n",
    "    def function_wrapper(*wrapper_args):\n",
    "        ncalls[0] += 1\n",
    "        return function(*(wrapper_args + args))\n",
    "\n",
    "    return ncalls, function_wrapper \n",
    "\n",
    "def line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None,\n",
    "                       old_old_fval=None, args=(), c1=1e-4, c2=0.9, amax=50):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x,*args)\n",
    "        Objective function.\n",
    "    myfprime : callable f'(x,*args)\n",
    "        Objective function gradient.\n",
    "    xk : ndarray\n",
    "        Starting point.\n",
    "    pk : ndarray\n",
    "        Search direction.\n",
    "    gfk : ndarray, optional\n",
    "        Gradient value for x=xk (xk being the current parameter\n",
    "        estimate). Will be recomputed if omitted.\n",
    "    old_fval : float, optional\n",
    "        Function value for x=xk. Will be recomputed if omitted.\n",
    "    old_old_fval : float, optional\n",
    "        Function value for the point preceding x=xk\n",
    "    args : tuple, optional\n",
    "        Additional arguments passed to objective function.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float or None\n",
    "        Alpha for which ``x_new = x0 + alpha * pk``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    fc : int\n",
    "        Number of function evaluations made.\n",
    "    gc : int\n",
    "        Number of gradient evaluations made.\n",
    "    new_fval : float or None\n",
    "        New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    old_fval : float\n",
    "        Old function value ``f(x0)``.\n",
    "    new_slope : float or None\n",
    "        The local slope along the search direction at the\n",
    "        new value ``<myfprime(x_new), pk>``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "    fc = [0]\n",
    "    gc = [0]\n",
    "    gval = [None]\n",
    "\n",
    "    def phi(alpha):\n",
    "        fc[0] += 1\n",
    "        return f(xk + alpha * pk, *args)\n",
    "\n",
    "    if isinstance(myfprime, tuple):\n",
    "        def derphi(alpha):\n",
    "            fc[0] += len(xk) + 1\n",
    "            eps = myfprime[1]\n",
    "            fprime = myfprime[0]\n",
    "            newargs = (f, eps) + args\n",
    "            gval[0] = fprime(xk + alpha * pk, *newargs)  # store for later use\n",
    "            return np.dot(gval[0], pk)\n",
    "    else:\n",
    "        fprime = myfprime\n",
    "\n",
    "        def derphi(alpha):\n",
    "            gc[0] += 1\n",
    "            gval[0] = fprime(xk + alpha * pk, *args)  # store for later use\n",
    "            return np.dot(gval[0], pk)\n",
    "\n",
    "    if gfk is None:\n",
    "        gfk = fprime(xk, *args)\n",
    "    derphi0 = np.dot(gfk, pk)\n",
    "\n",
    "    alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n",
    "            phi, derphi, old_fval, old_old_fval, derphi0, c1, c2, amax)\n",
    "\n",
    "    if derphi_star is None:\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "    else:\n",
    "        # derphi_star is a number (derphi) -- so use the most recently\n",
    "        # calculated gradient used in computing it derphi = gfk*pk\n",
    "        # this is the gradient at the next step no need to compute it\n",
    "        # again in the outer loop.\n",
    "        derphi_star = gval[0]\n",
    "\n",
    "    return alpha_star, fc[0], gc[0], phi_star, old_fval, derphi_star\n",
    "\n",
    "\n",
    "def scalar_search_wolfe2(phi, derphi=None, phi0=None,\n",
    "                         old_phi0=None, derphi0=None,\n",
    "                         c1=1e-4, c2=0.9, amax=50):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\n",
    "\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable f(x)\n",
    "        Objective scalar function.\n",
    "    derphi : callable f'(x), optional\n",
    "        Objective function derivative (can be None)\n",
    "    phi0 : float, optional\n",
    "        Value of phi at s=0\n",
    "    old_phi0 : float, optional\n",
    "        Value of phi at previous point\n",
    "    derphi0 : float, optional\n",
    "        Value of derphi at s=0\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha_star : float or None\n",
    "        Best alpha, or None if the line search algorithm did not converge.\n",
    "    phi_star : float\n",
    "        phi at alpha_star\n",
    "    phi0 : float\n",
    "        phi at 0\n",
    "    derphi_star : float or None\n",
    "        derphi at alpha_star, or None if the line search algorithm\n",
    "        did not converge.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Uses the line search algorithm to enforce strong Wolfe\n",
    "    conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
    "    1999, pg. 59-60.\n",
    "\n",
    "    For the zoom phase it uses an algorithm by [...].\n",
    "\n",
    "    \"\"\"\n",
    "    ##print('CALLING scalar_search_wolfe2')\n",
    "    if phi0 is None:\n",
    "        phi0 = phi(0.)\n",
    "\n",
    "    if derphi0 is None and derphi is not None:\n",
    "        derphi0 = derphi(0.)\n",
    "\n",
    "    alpha0 = 0\n",
    "    if old_phi0 is not None and derphi0 != 0:\n",
    "        alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
    "    else:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 < 0:\n",
    "        alpha1 = 1.0\n",
    "\n",
    "    if alpha1 == 0:\n",
    "        # This shouldn't happen. Perhaps the increment has slipped below\n",
    "        # machine precision?  For now, set the return variables skip the\n",
    "        # useless while loop, and raise warnflag=2 due to possible imprecision.\n",
    "        alpha_star = None\n",
    "        phi_star = phi0\n",
    "        phi0 = old_phi0\n",
    "        derphi_star = None\n",
    "\n",
    "    phi_a1 = phi(alpha1)\n",
    "    #derphi_a1 = derphi(alpha1)  evaluated below\n",
    "\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "\n",
    "    ##print('phi0:', phi0)\n",
    "    ##print('derphi0:', derphi0)\n",
    "    \n",
    "    i = 1\n",
    "    maxiter = 10\n",
    "    ##print('alpha0:', alpha0)\n",
    "    ##print('alpha1:', alpha1)\n",
    "    ##print('phi_a0:', phi_a0)\n",
    "    ##print('derphi_a0:', derphi_a0)\n",
    "    ##print('phi_a1:', phi_a1)\n",
    "    \n",
    "    for i in xrange(maxiter):\n",
    "        if alpha1 == 0:\n",
    "            break\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or \\\n",
    "           ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha0, alpha1, phi_a0,\n",
    "                              phi_a1, derphi_a0, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            ##print('ZOOM1')\n",
    "            ##print('alpha star:', alpha_star)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= -c2*derphi0):\n",
    "            alpha_star = alpha1\n",
    "            phi_star = phi_a1\n",
    "            derphi_star = derphi_a1\n",
    "            ##print('NO ZOOM')\n",
    "            break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star = \\\n",
    "                        _zoom(alpha1, alpha0, phi_a1,\n",
    "                              phi_a0, derphi_a1, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            ##print('ZOOM2')\n",
    "            ##print('alpha star:', alpha_star)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1   # increase by factor of two on each iteration\n",
    "        i = i + 1\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        warn('The line search algorithm did not converge', LineSearchWarning)\n",
    "\n",
    "    return alpha_star, phi_star, phi0, derphi_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a cubic polynomial that goes through the\n",
    "    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n",
    "\n",
    "    If no minimizer can be found return None\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a quadratic polynomial that goes through\n",
    "    the points (a,fa), (b,fb) with derivative at a of fpa,\n",
    "\n",
    "    \"\"\"\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "          phi, derphi, phi0, derphi0, c1, c2):\n",
    "    \"\"\"\n",
    "    Part of the optimization algorithm in `scalar_search_wolfe2`.\n",
    "    \"\"\"\n",
    "\n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        # interpolate to find a trial step length between a_lo and\n",
    "        # a_hi Need to choose interpolation here.  Use cubic\n",
    "        # interpolation and then if the result is within delta *\n",
    "        # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "        # then use quadratic interpolation, if the result is still too\n",
    "        # close, then use bisection\n",
    "\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "\n",
    "        # minimizer of cubic interpolant\n",
    "        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n",
    "        #\n",
    "        # if the result is too close to the end points (or out of the\n",
    "        # interval) then use quadratic interpolation with phi_lo,\n",
    "        # derphi_lo and phi_hi if the result is stil too close to the\n",
    "        # end points (or out of the interval) then use bisection\n",
    "\n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                            a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "\n",
    "        # Check new value of a_j\n",
    "\n",
    "        phi_aj = phi(a_j)\n",
    "        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if abs(derphi_aj) <= -c2*derphi0:\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            break\n",
    "    return a_star, val_star, valprime_star       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction: [ -307.6   531.6 -2845.4   682. ]\n",
      "Iteration: 1\n",
      "Function: 442.474051162\n",
      "Direction: [  42.40218069 -783.38073217 -386.63240946   58.93703941]\n",
      "Iteration: 2\n",
      "Function: 142.500722363\n",
      "Direction: [-220.94997259  -79.32615605   -3.99170367   20.32333431]\n",
      "Iteration: 3\n",
      "Function: 92.0398423348\n",
      "Direction: [ 0.46599912 -0.40586723  2.01028382  8.58334842]\n",
      "Iteration: 4\n",
      "Function: 91.8866732429\n",
      "Direction: [-0.06322159 -0.03761071  0.07866012 -0.15000061]\n",
      "Iteration: 5\n",
      "Function: 73.2739069075\n",
      "Direction: [-0.40650296 -0.17990893  0.79534595 -1.41658997]\n",
      "Iteration: 6\n",
      "Function: 40.3961234354\n",
      "Direction: [-0.24918507 -0.1021068   0.6463075  -0.95620369]\n",
      "Iteration: 7\n",
      "Function: 17.3481567586\n",
      "Direction: [ 0.04413264 -0.02200671 -0.12947434  0.37051613]\n",
      "Iteration: 8\n",
      "Function: 7.20222175921\n",
      "Direction: [-0.02759465 -0.02727373  0.08637637 -0.02294894]\n",
      "Iteration: 9\n",
      "Function: 4.19388878528\n",
      "Direction: [-0.02431569 -0.02825327  0.10028049 -0.00758147]\n",
      "Iteration: 10\n",
      "Function: 2.81682735021\n",
      "Direction: [ 0.00464724 -0.00173745  0.00892059  0.02024973]\n",
      "Iteration: 11\n",
      "Function: 2.73845203815\n",
      "Direction: [ 0.00786889  0.00341658  0.00524035  0.00486178]\n",
      "Iteration: 12\n",
      "Function: 2.71521334103\n",
      "Direction: [ 0.02981519  0.01504538  0.00893215  0.00766096]\n",
      "Iteration: 13\n",
      "Function: 2.67531390383\n",
      "Direction: [ 0.07416713  0.03748215  0.01277121  0.00961973]\n",
      "Iteration: 14\n",
      "Function: 2.60233290614\n",
      "Direction: [ 0.32138374  0.15947933  0.04145158  0.0278193 ]\n",
      "Iteration: 15\n",
      "Function: 2.05577946329\n",
      "Direction: [ 0.32250882  0.1753292   0.04499895 -0.0029227 ]\n",
      "Iteration: 16\n",
      "Function: 2.02424896043\n",
      "Direction: [ 0.01403651  0.01963605  0.00327645 -0.02439405]\n",
      "Iteration: 17\n",
      "Function: 1.96448555896\n",
      "Direction: [ 0.03714284  0.0494176  -0.00195572 -0.03897508]\n",
      "Iteration: 18\n",
      "Function: 1.85836200826\n",
      "Direction: [ 0.03454979  0.04610714  0.0022623  -0.03035663]\n",
      "Iteration: 19\n",
      "Function: 1.71287638813\n",
      "Direction: [ 0.01573981  0.02233811  0.00560727 -0.01084322]\n",
      "Iteration: 20\n",
      "Function: 1.6654072535\n",
      "Direction: [ 0.01771757  0.02263544  0.01003271 -0.00439973]\n",
      "Iteration: 21\n",
      "Function: 1.61809753564\n",
      "Direction: [ 0.05735207  0.07386287  0.04036805 -0.00222367]\n",
      "Iteration: 22\n",
      "Function: 1.53459318759\n",
      "Direction: [ 0.09116672  0.11793585  0.07526243  0.01062059]\n",
      "Iteration: 23\n",
      "Function: 1.39411157908\n",
      "Direction: [ 0.07385416  0.11093256  0.09074504  0.02623444]\n",
      "Iteration: 24\n",
      "Function: 1.18590392763\n",
      "Direction: [ 0.07631167  0.09219206  0.08338056  0.03829831]\n",
      "Iteration: 25\n",
      "Function: 1.00353096034\n",
      "Direction: [ 0.03312762  0.10441427  0.16719522  0.11074597]\n",
      "Iteration: 26\n",
      "Function: 0.768025523613\n",
      "Direction: [ 0.03426229  0.04582944  0.06307995  0.07283224]\n",
      "Iteration: 27\n",
      "Function: 0.520905072687\n",
      "Direction: [ 0.09522669  0.13592304  0.16912368  0.12793495]\n",
      "Iteration: 28\n",
      "Function: 0.433054804878\n",
      "Direction: [ 0.0458478   0.0767083   0.1167742   0.11571411]\n",
      "Iteration: 29\n",
      "Function: 0.313478362636\n",
      "Direction: [ 0.0172682   0.0386313   0.0678903   0.09827252]\n",
      "Iteration: 30\n",
      "Function: 0.184426453604\n",
      "Direction: [ 0.05610362  0.10120898  0.14683135  0.17866384]\n",
      "Iteration: 31\n",
      "Function: 0.147701883813\n",
      "Direction: [ 0.03466806  0.0577507   0.10329048  0.14204969]\n",
      "Iteration: 32\n",
      "Function: 0.105105564792\n",
      "Direction: [ 0.01401253  0.02695687  0.04457315  0.08550066]\n",
      "Iteration: 33\n",
      "Function: 0.0534630331903\n",
      "Direction: [ 0.01536074  0.03365714  0.06978836  0.11236525]\n",
      "Iteration: 34\n",
      "Function: 0.0249450421033\n",
      "Direction: [ 0.00541248  0.01091249  0.02297686  0.04648393]\n",
      "Iteration: 35\n",
      "Function: 0.013966255217\n",
      "Direction: [ 0.03691913  0.07096385  0.12343497  0.21947781]\n",
      "Iteration: 36\n",
      "Function: 0.00688583606622\n",
      "Direction: [ 0.00609578  0.01143337  0.02127417  0.0437844 ]\n",
      "Iteration: 37\n",
      "Function: 0.0023402978886\n",
      "Direction: [ 0.00598906  0.01189057  0.02505998  0.04887515]\n",
      "Iteration: 38\n",
      "Function: 0.000467017223064\n",
      "Direction: [ 0.000757    0.00226304  0.00514782  0.01280641]\n",
      "Iteration: 39\n",
      "Function: 0.000189979784788\n",
      "Direction: [ 0.00203937  0.00382726  0.0071138   0.01309083]\n",
      "Iteration: 40\n",
      "Function: 4.04158908823e-06\n",
      "Direction: [ -8.56423126e-05  -3.56583386e-04  -5.94314600e-04  -1.07470556e-03]\n",
      "Iteration: 41\n",
      "Function: 4.61044056325e-07\n",
      "Direction: [  1.46672559e-05   9.12109872e-05   1.60644215e-04   3.22116944e-04]\n",
      "Iteration: 42\n",
      "Function: 3.226025125e-09\n",
      "Direction: [  1.21606947e-05   2.23111079e-05   4.47410865e-05   8.81052936e-05]\n",
      "Iteration: 43\n",
      "Function: 6.50589940008e-11\n",
      "Direction: [  1.98033672e-07   4.24502228e-07   1.09336082e-06   2.90417056e-06]\n",
      "Iteration: 44\n",
      "Function: 1.64123695802e-13\n",
      "Direction: [  2.29306259e-08   5.57399773e-08   1.20833987e-07   2.77354227e-07]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 45\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin_bfgs(rosen, x0, rosen_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
